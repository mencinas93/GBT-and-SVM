{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3860482",
   "metadata": {},
   "source": [
    "Problem #5 (65 points)\n",
    "For this problem, you need to use the built-in sklearn digits dataset. You can load this data using\n",
    "\n",
    "        from sklearn.datasets import load_digits\n",
    "  digits = load_digits()\n",
    "\n",
    "(data will be stored in digits.data and desired output will be stored in digits.target)\n",
    "\n",
    "Divide the data into training and test sets using train_test_split and random_state=0\n",
    "\n",
    "1.\tTrain a Gradient Boosted Regression Trees classifier and optimize its performance on this data. Using 4-fold cross validation, design your experiment to learn the best values for the following parameters using 4-fold cross validation: n_estimators, learning_rates, and max_depth \n",
    "a.\tAnalyze the results of the classifier using its optimal parameters and comment on its generalization capability by comparing the accuracy on the training, validation, and test data.\n",
    "b.\tIdentify the number of misclassified test samples from each class. Explain why some classes are easier to classifier than others.\n",
    "c.\tVisualize and explain the relevant features identified by the classifier\n",
    "Create a white 8x8 image that represents the original 64 features. Map each identified relevant feature to this 2D image and display it using a color or a grey scale that reflects its importance (as illustrated in the lecture)\n",
    "\n",
    "2.\tTrain a Support Vector Machines classifier and optimize its performance on this data. \n",
    "Using 4-fold cross validation, design your experiment to learn the best values for the following parameters \n",
    "i.\tData normalization: no preprocessing vs. StandardScaler\n",
    "ii.\tKernel: Linear vs. RBF\n",
    "iii.\tThe regularization parameter C: 4 different values. \n",
    "a.\t Analyze the results of the classifier using its optimal parameters and comment on its generalization capability by comparing the accuracy on the training, validation, and test data.\n",
    "b.\tIdentify the number of misclassified test samples from each class.\n",
    "\n",
    "3.\tAnalyze the correlation between the output of the 2 classifiers by displaying the predict_proba of SVM vs. predict_proba of GBRT (using test data). Using these scatter plots (only for classes “0”, “3”, and “7”), identify (if available) the following 3 groups\n",
    "c.\tG-1: Samples that are easy to classify correctly by the SVM, but hard to classify by GBRT\n",
    "d.\tG-2: Samples that are easy to classify correctly by the GBRT, but hard to classify by SVM\n",
    "e.\tG-3: Samples that are hard to classify correctly by both methods\n",
    "For each group, display few samples (as images) and identify any common features among them.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d5a64cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold, cross_val_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "09b5af38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "04702ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "49c022e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2007f76",
   "metadata": {},
   "source": [
    "80 percent of data used for train and 20 percent of data used for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7ccc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = [10, 50, 100]\n",
    "learning_rates = [0.01, 0.1, 0.2]\n",
    "max_depths = [1, 3, 5]\n",
    "\n",
    "# Set up 4-fold cross-validation\n",
    "kf = KFold(n_splits=4, shuffle=True, random_state=0)\n",
    "\n",
    "# To store results\n",
    "results = []\n",
    "\n",
    "# Iterate through each combination of hyperparameters\n",
    "for n_estimator in n_estimators:\n",
    "    for learning_rate in learning_rates:\n",
    "        for max_depth in max_depths:\n",
    "            # Initialize GBRT with current parameters\n",
    "            gbrt = GradientBoostingClassifier(\n",
    "                n_estimators=n_estimator, \n",
    "                learning_rate=learning_rate, \n",
    "                max_depth=max_depth,\n",
    "                random_state=0\n",
    "            )\n",
    "\n",
    "            # Measure the start time\n",
    "            start_time = time.time()\n",
    "\n",
    "            # Perform cross-validation on training data\n",
    "            cv_scores = cross_val_score(gbrt, X_train, y_train, cv=kf, n_jobs=-1)\n",
    "\n",
    "            # Train model on entire training set\n",
    "            gbrt.fit(X_train, y_train)\n",
    "\n",
    "            # Calculate training accuracy\n",
    "            train_accuracy = accuracy_score(y_train, gbrt.predict(X_train))\n",
    "\n",
    "            # Measure the end time and calculate elapsed time\n",
    "            elapsed_time = time.time() - start_time\n",
    "\n",
    "            # Store results\n",
    "            results.append({\n",
    "                'n_estimators': n_estimator,\n",
    "                'learning_rate': learning_rate,\n",
    "                'max_depth': max_depth,\n",
    "                'avg_training_accuracy': train_accuracy,\n",
    "                'avg_cv_accuracy': np.mean(cv_scores),\n",
    "                'time_taken': elapsed_time\n",
    "            })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e48e4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
